{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/craigschindler/numerai/blob/main/NeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaNpcfNkKUg_"
      },
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install numerapi\n",
        "import numerapi\n",
        "import sklearn.linear_model\n",
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy import stats\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "sns.set_context(\"talk\")\n",
        "style.use('fivethirtyeight')\n",
        "\n",
        "import graphviz\n",
        "import pydot\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uUN-aE_08kW"
      },
      "source": [
        "## The datasets\n",
        "\n",
        "### Datasets \n",
        "*   `training_data` is used to train your model\n",
        "*   `tournament_data` is used to evaluate your model\n",
        "\n",
        "### Column descriptions\n",
        "*   id: a randomized id that corresponds to a stock \n",
        "*   era: a period of time\n",
        "*   data_type: either `train`, `validation`, `test`, or `live` \n",
        "*   feature_*: abstract financial features of the stock \n",
        "*   target: abstract measure of stock performance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyB8NsbUNNWR"
      },
      "source": [
        "# load the training data set\n",
        "training_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\")\n",
        "training_data[\"erano\"] = training_data.era.str.slice(3).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSVN4hpZEYDW"
      },
      "source": [
        "# load the tournament dataset and validation data set\n",
        "tournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\")\n",
        "validation_data = tournament_data.loc[tournament_data['data_type'] == \"validation\"].copy()\n",
        "validation_data[\"erano\"] = validation_data.era.str.slice(3).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N093u9oEYDW"
      },
      "source": [
        "tournament_data.loc[tournament_data['data_type'] == \"test\"].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuYOUM-3EYDX"
      },
      "source": [
        "len(tournament_data.loc[tournament_data['data_type'] == \"live\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Fcz-PpHYez"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESazJMMzN--g"
      },
      "source": [
        "# find only the feature columns\n",
        "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bmbqi4YKY2Z"
      },
      "source": [
        "# select those columns out of the training dataset and validation dataset\n",
        "training_features = training_data[feature_cols]\n",
        "validation_features = validation_data[feature_cols]\n",
        "training_targets = training_data[\"target\"]\n",
        "validation_targets = validation_data[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXBF15h-EYDa"
      },
      "source": [
        "#set hyperparameters for neural network\n",
        "hyperparameters = {}\n",
        "nn_training_data = {}\n",
        "nn_validation_data = {}\n",
        "#######################################\n",
        "hyperparameters[\"learning_rate\"] = 0.001\n",
        "hyperparameters[\"hidden_layer_1_units\"] = 256\n",
        "hyperparameters[\"hidden_layer_2_units\"] = 128\n",
        "hyperparameters[\"hidden_layer_3_units\"] = 64\n",
        "hyperparameters[\"dropout_layer_1_rate\"] = 0.1\n",
        "hyperparameters[\"dropout_layer_2_rate\"] = 0.1\n",
        "hyperparameters[\"dropout_layer_3_rate\"] = 0.1\n",
        "hyperparameters[\"batch_size\"] = 128\n",
        "hyperparameters[\"epochs\"] = 15\n",
        "nn_training_data[\"examples\"] = training_features.to_numpy()\n",
        "nn_training_data[\"targets\"] = training_targets.to_numpy()\n",
        "nn_validation_data[\"examples\"] = validation_features.to_numpy()\n",
        "nn_validation_data[\"targets\"] = validation_targets.to_numpy()\n",
        "es = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss')\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, verbose=1, mode='min')\n",
        "callbacks = [es, lr_scheduler]\n",
        "#######################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZALW3BKEYDa"
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "model.add(Dense(hyperparameters[\"hidden_layer_1_units\"], input_dim=310)) #there are 310 features, i.e. inputs\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activations.relu))\n",
        "model.add(Dropout(hyperparameters[\"dropout_layer_1_rate\"]))\n",
        "model.add(Dense(hyperparameters[\"hidden_layer_2_units\"]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activations.relu))\n",
        "model.add(Dropout(hyperparameters[\"dropout_layer_2_rate\"]))\n",
        "model.add(Dense(hyperparameters[\"hidden_layer_3_units\"]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activations.relu))\n",
        "model.add(Dropout(hyperparameters[\"dropout_layer_3_rate\"]))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(activations.linear))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"learning_rate\"]) #default adam learning rate is learning_rate=0.001\n",
        "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt)\n",
        "history = model.fit(nn_training_data[\"examples\"], nn_training_data[\"targets\"], epochs=hyperparameters[\"epochs\"], batch_size=hyperparameters[\"batch_size\"],validation_data=(nn_validation_data[\"examples\"],nn_validation_data[\"targets\"]),callbacks=callbacks)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOQZf2XEYDa"
      },
      "source": [
        "\n",
        "#model.save('/content/drive/My Drive/Colab Notebooks/mymdl') #UNCOMMENT TO SAVE MODEL\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/mymdl') #UNCOMMENT TO LOAD MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sALFyL2VEYDb"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WviEXLzCEYDb"
      },
      "source": [
        "## Calculate Validation Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MtNAzQUlEYDb"
      },
      "source": [
        "# The models should be scored based on the rank-correlation (spearman) with the target\n",
        "def spearman_corr(y_true, y_pred):\n",
        "    #return stats.spearmanr(y_true,y_pred)[0]\n",
        "    ranked_predictions = y_pred.rank(pct=True, method=\"first\")\n",
        "    return np.corrcoef(y_true, ranked_predictions)[0,1]\n",
        "def pearson_corr(y_true, y_pred):\n",
        "    return np.corrcoef(y_true, y_pred)[0,1]\n",
        "\n",
        "df = validation_data\n",
        "df_features = validation_features\n",
        "\n",
        "df[\"predictions\"] = model.predict(df_features.to_numpy())\n",
        "\n",
        "corr_array = []\n",
        "\n",
        "for erano in df.erano.unique():\n",
        "    tdf = df.loc[df['erano'] == erano]\n",
        "    corr_array.append(spearman_corr(tdf[\"target\"], tdf[\"predictions\"]))\n",
        "plt.hist(corr_array)\n",
        "plt.title(\"histogram of validation corr by era\")\n",
        "plt.show()\n",
        "print(\"sharpe ratio: \" + str(np.mean(corr_array)/np.std(corr_array)))\n",
        "print(\"mean of validation corrs by era: \" + str(np.mean(corr_array)))\n",
        "print(\"std of validation corrs by era: \" + str(np.std(corr_array)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxx5pLIGIUvN"
      },
      "source": [
        "## 4. Generate your first predictions\n",
        "Now that we have a trained model, we can use it to make predictions on the tournament data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIcmOiJYSmJI"
      },
      "source": [
        "# select the feature columns from the tournament data\n",
        "live_features = tournament_data[feature_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4CoaCgbKcSN"
      },
      "source": [
        "# predict the target on the live features\n",
        "predictions = model.predict(live_features.to_numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L4BFeAzTTDe"
      },
      "source": [
        "# predictions must have an `id` column and a `prediction_kazutsugi` column\n",
        "predictions_df = tournament_data[\"id\"].to_frame()\n",
        "predictions_df[\"prediction\"] = predictions\n",
        "predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0sgtPiLDys"
      },
      "source": [
        "## 5. Make your first submission\n",
        "To enter the tournament, we must submit the predictions back to Numerai. We will use the `numerapi` library to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEfqpxcEWDdK"
      },
      "source": [
        "# Get your API keys and model_id from https://numer.ai/submit\n",
        "# public_id = \"<YOUR PUBLIC ID HERE\"\n",
        "# secret_key = \"<YOUR SECRET KEY HERE>\"\n",
        "# model_id = \"<YOUR MODEL ID HERE>\"\n",
        "# napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAIJHaoW3VU"
      },
      "source": [
        "# Upload your predictions\n",
        "predictions_df.to_csv(\"predictions.csv\", index=False)\n",
        "files.download('predictions.csv')\n",
        "# submission_id = napi.upload_predictions(\"predictions.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jc2NgEgQEKF"
      },
      "source": [
        "# Done ðŸš€\n",
        "Good job! You just made your first submission on Numerai!\n",
        "\n",
        "Head back over to https://numer.ai/submit to continue."
      ]
    }
  ]
}